{
    "header": {
        "brand": "BRAND",
        "about": "ABOUT",
        "works": "WORKS",
        "transmissions": "TRANSMISSIONS",
        "talk_to_us": "TALK TO US"
    },
    "hero": {
        "title": "FROM SET TO SERVER.",
        "subtitle": "10 YEARS OF CRAFT.",
        "scramble": "NOW GENERATIVE.",
        "description": "A NEW DIVISION BY BRICK.\nFROM ZERO TO ALL SINCE 2016."
    },
    "philosophy": {
        "belief_label": "The Belief",
        "raw": {
            "title": "RAW.",
            "text": "AI creates infinite pixels and patterns. But it cannot create intent. It is just a resource."
        },
        "noise": {
            "title": "NOISE.",
            "text": "Without a human hand, generative models are just mathematical coincidence. We provide the vision."
        },
        "direct": {
            "title": "WE DIRECT THE INTELLIGENCE.",
            "text": "The machine is the brush. The database is the paint. We are still the artists."
        }
    },
    "works": {
        "inheritance": {
            "title": "INHERITANCE",
            "subtitle": "FULL GENERATIVE",
            "desc": "When the location doesn't exist. 100% Neural Rendering.",
            "longDesc": "Inheritance is a testament to directed hallucination. Our artists trained custom LoRAs on brutalist architectural blueprints to guide the Stable Diffusion XL model. We curated 4,000 frames where organic decay meets concrete, ensuring temporal consistency through ControlNet depth maps while maintaining the director's original vision of isolation."
        },
        "shift": {
            "title": "WE CAN SELL ANYTHING",
            "subtitle": "HYBRID EXTENSION",
            "desc": "Expanding the set beyond physical limits. Seamless VFX.",
            "longDesc": "For 'We Can Sell Anything', the challenge was to marry physical sets with infinite digital horizons. We utilized a proprietary in-painting pipeline that tracks live-action camera data (Alexa Mini) and feeds it into a generative fill model. The result is a mathematically perfect extension of the set, lighting, and grain, directed precisely by the cinematographer's lens choices."
        },
        "anima": {
            "title": "AUTOBOL",
            "subtitle": "STYLE TRANSFER",
            "desc": "Turning live action into branded art using custom models.",
            "longDesc": "We reinterpreted standard broadcast footage through a custom-trained style transfer model using Ebsynth and ControlNet. The goal was not random abstraction, but a specific 'kinetic sculpture' aesthetic defined by the art director. The AI acted as the brush, preserving the players' identity and 60fps fluidity while completely transforming the texture of reality."
        },
        "void": {
            "title": "VOID GAZING",
            "subtitle": "DATA VISUALIZATION",
            "desc": "Translating cosmic radiation into visible spectrums.",
            "longDesc": "A data-driven installation that visualizes real-time cosmic radiation data via the NASA Open API. We wrote Python scripts to parse numerical noise into fluid dynamics parameters within TouchDesigner. The AI then textures this simulation in real-time, effectively allowing the audience to 'see' the invisible universe through a human-curated aesthetic lens."
        },
        "urban": {
            "title": "URBAN REEF",
            "subtitle": "PROCEDURAL ENV",
            "desc": "Growing cities like coral. Biological algorithms applied.",
            "longDesc": "Urban Reef explores bio-mimicry in architecture. Using differential growth algorithms in Houdini, we 'grew' city structures that seek light like coral. These procedural meshes were then texturized using AI upscaling, creating a vision of a city that feels grown rather than built, challenging traditional architectural design processes."
        },
        "silent": {
            "title": "SILENT ECHO",
            "subtitle": "AUDIO REACTIVE",
            "desc": "A visual narrative driven entirely by sub-bass frequencies.",
            "longDesc": "In Silent Echo, the music drives the machine. We built a system in Unreal Engine 5 where visual elements are triggered directly by audio analysis. Sub-bass frequencies dictate geometry displacement, while high-hats trigger generative lighting events. It is a synesthetic experience where the artist's sound directly sculpts the digital world."
        }
    },
    "transmissions": {
        "log_001": {
            "title": "THE LATENT SPACE IS A LOCATION",
            "excerpt": "Why we stopped scouting physical ruins and started training LoRAs on brutalist blueprints.",
            "content_p1": "We tend to think of generative models as engines of creation. Input prompt, output image. A linear manufacturing process. But this metaphor is insufficient for high-end production. At Brick, we treat the model not as a factory, but as a territory.",
            "section_title": "The Topography of Noise",
            "content_p2": "Stable Diffusion XL does not \"draw\". It denoises. It subtracts chaos to reveal order. This implies that the image already exists within the high-dimensional noise, mathematically waiting to be uncovered.",
            "quote": "\"We do not build the set. We navigate to the coordinates where the set is statistically most likely to exist.\""
        },
        "log_002": {
            "title": "MOTION VECTORS IN STYLE TRANSFER",
            "excerpt": "Solving the flickering problem in diffusion models. How we use optical flow to enforce temporal consistency.",
            "content_p1": "The jitter. The flicker. The \"boiling\" texture. This is the hallmark of raw AI video. It reveals the frame-by-frame independence of the diffusion process. For <0>Project: Anima</0>, this artifact was unacceptable.",
            "content_p2": "Our solution involved extracting optical flow maps from the source footage using Nuke. These motion vectors act as a temporal skeleton."
        },
        "log_003": {
            "title": "INTENTIONAL GLITCH: THE HUMAN SIGNATURE",
            "excerpt": "When perfection is the error. Injecting noise back into the clean output of commercial models to reclaim the 'cinema' feel.",
            "content_p1": "Modern models are converging towards a \"mid-journey mean\". A polished, plastic aesthetic that screams \"synthetic\". Paradoxically, to make AI imagery feel real, we must break it.",
            "quote": "The artifact is the art."
        }
    },
    "common": {
        "return_surface": "RETURN TO SURFACE",
        "return_index": "RETURN TO INDEX",
        "system_status": "SYSTEM_STATUS",
        "online": "ONLINE",
        "secure_connection": "SECURE CONNECTION",
        "loading": "LOADING SYSTEM..."
    },
    "legacy": {
        "title": "BACKED BY BRICK.",
        "title_mobile_br": "BACKED <br /> BY BRICK.",
        "text": "This isn't a beta test. This is a new lens from a production house with 10 years of experience.",
        "trusted_by": "Trusted By"
    },
    "footer": {
        "complex_problem": "Have a complex problem?",
        "we_have_intelligence": "WE HAVE THE INTELLIGENCE.",
        "talk_to_us": "TALK TO US",
        "network": "Network",
        "rights_reserved": "All Rights Reserved.",
        "generative_division": "The Generative Division",
        "system_admin": "SYSTEM_ADMIN"
    },
    "about": {
        "origin": "SYSTEM_ORIGIN",
        "est": "EST. 2016",
        "title_primary": "FORGED IN",
        "title_highlight": "OLD SCHOOL",
        "title_secondary": "VFX.",
        "description": "Before we ever wrote a prompt, we spent 7 years pushing pixels in Nuke and Maya. We understand light, composition, and storytelling because we built them by hand for a decade. We didn't adopt AI to replace the craft. We adopted it to break the speed limit.",
        "core_modules": "CORE_MODULES",
        "modules": {
            "cinematography": {
                "title": "SYNTHETIC_CINEMATOGRAPHY",
                "status": "INSTALLED",
                "desc": "Video Generation // ComfyUI // 4K Upscaling"
            },
            "training": {
                "title": "MODEL_TRAINING",
                "status": "INSTALLED",
                "desc": "Custom LoRAs // Style Consistency // Fine-Tuning"
            },
            "architecture": {
                "title": "PIPELINE_ARCHITECTURE",
                "status": "INSTALLED",
                "desc": "Python Tooling // Automation // Render Farm"
            }
        },
        "manifesto": {
            "title": "THE ANTI-PROMPT MANIFESTO",
            "subtitle": "Why Magic Doesn't Scale",
            "cards": {
                "control": {
                    "title": "CONTROL > CHANCE",
                    "desc": "The 'perfect prompt' is a myth. Consistency comes from ControlNet, IP-Adapters, and Python scripts, not lucky words. We engineer our images; we don't wish for them."
                },
                "curation": {
                    "title": "CURATION IS CREATION",
                    "desc": "A model can generate 1,000 images in a minute. The art is knowing which one is wrong. Our directors curate with the same critical eye they used on film sets for 10 years."
                },
                "black_box": {
                    "title": "NO BLACK BOXES",
                    "desc": "We don't rely on closed web-interfaces. We build our own ComfyUI pipelines locally. This gives us pixel-level control over the latent space that 'magic buttons' can't provide."
                }
            }
        },
        "team": {
            "title": "UNIT_LEADERS // COMMAND",
            "roles": {
                "alex": "EXECUTIVE PRODUCER",
                "sarah": "CREATIVE DIRECTOR",
                "gabriel": "HEAD OF TECHNOLOGY",
                "marcus": "VFX SUPERVISOR"
            }
        }
    },
    "chat": {
        "reach_humans": "REACH_HUMANS",
        "manual_override": "MANUAL OVERRIDE PROTOCOLS",
        "status_online": "STATUS: ONLINE",
        "email_streams": "EMAIL_STREAMS",
        "voice_link": "VOICE_LINK",
        "network_nodes": "NETWORK_NODES",
        "mason_intro": "I AM MASON",
        "generative_core": "The Generative Core.",
        "state": "State:",
        "active": "ACTIVE",
        "idle": "IDLE",
        "placeholder": "ENTER COMMAND...",
        "execute": "EXECUTE",
        "initial_messages": {
            "online": "SYSTEM_ONLINE. I am MASON. I build the foundation of your reality.",
            "protocol": "Protocol initiated. Transmit your query for immediate processing."
        },
        "suggestions": {
            "philosophy": "What is the Monolith philosophy?",
            "audit": "Execute project audit.",
            "synthesis": "Initiate creative synthesis.",
            "humans": "Manual override: Speak to humans."
        }
    },
    "works_page": {
        "title": "PROJECT_DATABASE // Selected Works",
        "archive_index": "ARCHIVE_INDEX",
        "accessing": "ACESSING NEURAL DATABASE...",
        "entries_found": "ENTRIES FOUND.",
        "no_data": "NO DATA FOUND IN THIS SECTOR.",
        "protocols": "PROTOCOLS"
    },
    "transmissions_page": {
        "title": "NEURAL_LOGS",
        "incoming": "INCOMING DATA STREAMS...",
        "records": "RECORDS."
    },
    "project_modal": {
        "neural_active": "NEURAL_RENDER_ACTIVE",
        "static_preview": "STATIC_PREVIEW",
        "system_data": "System Data"
    }
}